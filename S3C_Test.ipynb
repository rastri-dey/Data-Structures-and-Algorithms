{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOM79EbTKlAHA7aVAeaj7n9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rastri-dey/Data-Structures-and-Algorithms/blob/main/S3C_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8HK8lfopmJro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Check Isomorphism"
      ],
      "metadata": {
        "id": "I3ltpiKoh1Yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ctypes\n",
        "import multiprocessing\n",
        "from functools import lru_cache\n",
        "from multiprocessing import Process\n",
        "\n",
        "import rustworkx as rx"
      ],
      "metadata": {
        "id": "NlVPHvQCiV3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcT8koPeYfzW",
        "outputId": "6f227425-5cc7-4f03-a070-6bb829851326"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "car_2\n"
          ]
        }
      ],
      "source": [
        "def remove_ids(label):\n",
        "    \"\"\"\n",
        "    Remove the id number from the label\n",
        "\n",
        "    Assumes that if an id is present, it is of the form _number, e.g. car_2\n",
        "    \"\"\"\n",
        "    if label is None:\n",
        "        return None\n",
        "    # hasattr: if an object(label) has a specific method or attribute\n",
        "    # isinstance: if object is an instance of class or subclass\n",
        "    if not (hasattr(label, 'name') and hasattr(label, 'label')):\n",
        "        if isinstance(label, dict) and 'label' in label:\n",
        "            return label['label']\n",
        "        if hasattr(label, 'name'):\n",
        "            under_index = label.name.rfind('_')\n",
        "            if under_index == -1:\n",
        "                return label.name\n",
        "            else:\n",
        "                return label.name[:under_index]\n",
        "        return label\n",
        "    under_index = label.name.rfind('_')\n",
        "    return '%s:%s' % (label.label, label.name[:under_index] if under_index > 0 else label.name)\n",
        "label = \"car_2\"\n",
        "new_label = remove_ids(label)\n",
        "print(new_label)  # Output: \"car\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@lru_cache\n",
        "def get_hierarchy_check(force_consistent_ids=False):\n",
        "    def hierarchy_check(label1, label2):\n",
        "        # If they have the same label, they are equal\n",
        "        if not force_consistent_ids:\n",
        "            label1 = remove_ids(label1)\n",
        "            label2 = remove_ids(label2)\n",
        "        if label1 == label2:\n",
        "            return True\n",
        "    return hierarchy_check"
      ],
      "metadata": {
        "id": "qg_gV0gv9Mni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# comparing the structure of two ASGs without regard to the specific names of their nodes and edges\n",
        "# In 2 graphs leaving node label ids or names, whether node & edge are same\n",
        "def compare_asgs(asg1, asg2):\n",
        "    return rx.digraph_is_isomorphic(asg1, asg2, id_order=False,\n",
        "                                    node_matcher=get_hierarchy_check(),\n",
        "                                    edge_matcher=get_hierarchy_check())"
      ],
      "metadata": {
        "id": "C6Ta-XDJAeI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dict_to_string(d: dict):\n",
        "    return str({key: d[key] for key in sorted(d.keys())})"
      ],
      "metadata": {
        "id": "PKM7YK0NC72r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_class_counts(asg):\n",
        "    \"\"\"\n",
        "    Returns a tuple of dictionaries (node_class_counts, edge_class_counts) with the class counts.\n",
        "\n",
        "    If two graphs are isomorphic, then they must have the same number of every node and edge label (here called \"class\")\n",
        "    This is used as a lightweight check as part of maybe_isomorphic to avoid running the more expensive algorithm.\n",
        "    \"\"\"\n",
        "    node_class_counts = {}\n",
        "    for node_index in asg.node_indices():\n",
        "        label = remove_ids(asg[node_index])\n",
        "        if label not in node_class_counts:\n",
        "            node_class_counts[label] = 0\n",
        "        node_class_counts[label] += 1\n",
        "    edge_class_counts = {}\n",
        "    for edge_index, (node1, node2, edge_data) in asg.edge_index_map().items():\n",
        "        label = remove_ids(edge_data)\n",
        "        if label not in edge_class_counts:\n",
        "            edge_class_counts[label] = 0\n",
        "        edge_class_counts[label] += 1\n",
        "    return dict_to_string(node_class_counts), dict_to_string(edge_class_counts)"
      ],
      "metadata": {
        "id": "9ctLtMLSS-pR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def maybe_isomorphic(asg1, asg2):\n",
        "    \"\"\"Check metrics which are necessary but not sufficient for isomorphism and cheap to compute\"\"\"\n",
        "    # if the graphs don't have the same size then they can't be isomorphic\n",
        "    if asg1.num_nodes() != asg2.num_nodes() or asg1.num_edges() != asg2.num_edges():\n",
        "        return False\n",
        "    else:\n",
        "        # if they don't have the same number of nodes and edges with equivalent labels, they can't be isomorphic\n",
        "        asg1_class_counts = get_class_counts(asg1)\n",
        "        asg2_class_counts = get_class_counts(asg2)\n",
        "        if asg1_class_counts != asg2_class_counts:\n",
        "            return False\n",
        "    return True"
      ],
      "metadata": {
        "id": "BH9CFiNTWTIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def __is_isomorphic(asg1, asg2, value: multiprocessing.Value = None):\n",
        "    \"\"\"Checks isomorphism directly. For improved performance, check maybe_isomorphic first\"\"\"\n",
        "    result = compare_asgs(asg1, asg2)\n",
        "    if value is not None:\n",
        "        value.value = 1 if result else -1\n",
        "    return result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "collapsed": true,
        "id": "8ci6beusY29-",
        "outputId": "8656b352-fab5-4926-8c5f-7b7099e145f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'multiprocessing' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-d4362dd41c85>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m__is_isomorphic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mValue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Checks isomorphism directly. For improved performance, check maybe_isomorphic first\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompare_asgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'multiprocessing' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_isomorphic(asg1, asg2, timeout=-1, check_preconditions=True):\n",
        "    \"\"\"\n",
        "     Strict comparison assuming perfect equality. Can be run with timeout.\n",
        "     If check_preconditions, checks maybe_isomorphic first. That time is excluded from the timeout.\n",
        "\n",
        "     If timeout <=0, run without time limit. If timeout>0, run for at mose timeout seconds.\n",
        "     If no result is found within that time, None is returned.\n",
        "    \"\"\"\n",
        "    if check_preconditions and not maybe_isomorphic(asg1, asg2):\n",
        "        return False\n",
        "    if timeout <= 0:\n",
        "        is_iso = __is_isomorphic(asg1, asg2)\n",
        "    else:\n",
        "        return_value = multiprocessing.Value(ctypes.c_byte, 0)\n",
        "        p = Process(target=__is_isomorphic, args=(asg1, asg2, return_value))\n",
        "        p.start()\n",
        "        p.join(timeout)\n",
        "        if p.is_alive() or return_value.value == 0:\n",
        "            p.terminate()\n",
        "            p.join()\n",
        "            is_iso = None\n",
        "        else:\n",
        "            is_iso = return_value.value > 0\n",
        "    return is_iso"
      ],
      "metadata": {
        "id": "4yzvwTlCZwhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Cluster"
      ],
      "metadata": {
        "id": "ef0iIU-eiB2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging      # Log messages\n",
        "import os           # interacting with the operating system\n",
        "import sys          # interacting with the interpreter\n",
        "\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "\n",
        "from pipeline.Dataloader.dataloader_factory import DataLoaderFactory, DATALOADERS\n",
        "from utils.dataset import Dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "collapsed": true,
        "id": "u72HU81JiIb7",
        "outputId": "660efcc3-90da-48bc-ac77-0877d20683c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pipeline'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-cb1c54b18036>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader_factory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoaderFactory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATALOADERS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pipeline'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_cluster_args(arg_string):\n",
        "    parser = argparse.ArgumentParser(description=\"Arguments for cluster generation\")\n",
        "    parser.add_argument(\n",
        "        \"-dt\",\n",
        "        \"--dataset_type\",\n",
        "        type=str,\n",
        "        choices=[dataloader for dataloader in DATALOADERS],\n",
        "        required=False,\n",
        "        help=\"Dataset type\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-dp\",\n",
        "        \"--dataset_path\",\n",
        "        type=Path,\n",
        "        required=False,\n",
        "        help=\"Dataset folder path\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-sp\",\n",
        "        \"--sgs_path\",\n",
        "        type=Path,\n",
        "        required=False,\n",
        "        help=\"Path to save/load SGs\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-dsf\",\n",
        "        \"--dataset_file\",\n",
        "        type=Path,\n",
        "        required=False,\n",
        "        help=\"Location to store data set file. If file exists, process will not regenerate and will exit. If file does \"\n",
        "             \"not exist, then dataset_type, dataset_path, and sgs_path must be set.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-j\",\n",
        "        \"--threads\",\n",
        "        type=int,\n",
        "        required=False,\n",
        "        default=1,\n",
        "        help=\"Number of threads to use for various operations.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-max_per_thread\",\n",
        "        \"--max_per_thread\",\n",
        "        type=int,\n",
        "        required=False,\n",
        "        default=512,\n",
        "        help=\"Maximum number of graphs per thread. Larger groups will be split across threads and then merged.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-load_threaded\",\n",
        "        \"--load_threaded\",\n",
        "        type=bool,\n",
        "        required=False,\n",
        "        default=False,\n",
        "        help=\"Whether to load the SGs in a threaded manner.\"\n",
        "    )\n",
        "    parser.add_argument('-v', '--verbose', action='store_true')\n",
        "\n",
        "    return parser.parse_args(arg_string)"
      ],
      "metadata": {
        "id": "AiqHAhcAqfYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Parses the command-line arguments using the parse_cluster_args function.\n",
        "Checks if the dataset file exists.\n",
        "\n",
        "If the dataset file exists, it loads the dataset and extracts the scene graphs.\n",
        "extract SG from util/dataset.py\n",
        "Generates clusters based on the specified parameters.\n",
        "Generates clusters from util/dataset.py\n",
        "\n",
        "Saves the cluster information to a file.\n",
        "Optionally, generates visualizations of the clusters.\n",
        "If the dataset file does not exist, it prints an error message.\n",
        "'''\n",
        "def generate_clusters(arg_string):\n",
        "    args = parse_cluster_args(arg_string)\n",
        "    if args.verbose:\n",
        "        logging.basicConfig(level=logging.INFO)\n",
        "    if args.dataset_file is not None and os.path.exists(args.dataset_file):\n",
        "        # The below structure can be used to load the datasets created by this method.\n",
        "        # Note that the dataset path and sgs path args only need to be provided if the full file paths are needed\n",
        "        # Otherwise, the dataset contains sufficient information for most other analysis\n",
        "        dataset = Dataset.load_from_file(args.dataset_file, args.dataset_path, args.sgs_path)\n",
        "    else:\n",
        "        logging.info('Could not find dataset file %s, generating dataset' % args.dataset_file)\n",
        "        dataloader = DataLoaderFactory(args.dataset_type, args.dataset_path, sgs_path=args.sgs_path,\n",
        "                                       loader_type='Paths', shuffle=False)\n",
        "        dataset = Dataset(dataloader,\n",
        "                          threads=args.threads,\n",
        "                          max_per_thead=args.max_per_thread,\n",
        "                          load_threaded=args.load_threaded)\n",
        "        if args.dataset_file is not None:\n",
        "            logging.info('Saving dataset to file')\n",
        "            dataset.save_to_file(args.dataset_file, args.dataset_path, args.sgs_path)\n"
      ],
      "metadata": {
        "id": "shd5YUBWJSMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset: Set of clustered ASGs"
      ],
      "metadata": {
        "id": "A4yu7bZak9jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time # Provides various time-related functionalities\n",
        "from pathlib import Path, PosixPath # Offers object-oriented filesystem paths\n",
        "from utils.asg_compare import is_isomorphic, get_class_counts # Provides functions for comparing abstract syntax graphs (ASGs)\n",
        "import os # Allows interaction with the operating system\n",
        "import pickle # Used for object serialization and deserialization\n",
        "from rustworkx import networkx_converter # Facilitates conversion between NetworkX and RustGraph objects\n",
        "from functools import lru_cache # Provides higher-order functions and decorators\n",
        "import functools\n",
        "import json # Enables working with JSON data\n",
        "import multiprocessing # Supports parallel processing using multiple processors\n",
        "from multiprocessing import Pool # Represents a pool of worker processes for parallel execution\n",
        "from concurrent.futures import ProcessPoolExecutor as ProcessPoolExecutor\n",
        "from typing import Tuple, List, Dict\n",
        "\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "from pipeline.Dataloader.abstract_dataloader import AbstractDataloader\n",
        "\n",
        "import rustworkx as rx\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "collapsed": true,
        "id": "dESMgzhwlEeS",
        "outputId": "4daeb91b-d9f7-4028-cebd-1dd9d3215682"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'utils'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-61847e59b810>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;31m# Provides various time-related functionalities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPosixPath\u001b[0m \u001b[0;31m# Offers object-oriented filesystem paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masg_compare\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_isomorphic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_class_counts\u001b[0m \u001b[0;31m# Provides functions for comparing abstract syntax graphs (ASGs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m \u001b[0;31m# Allows interaction with the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m \u001b[0;31m# Used for object serialization and deserialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, name, base_class=None):\n",
        "        self.name = name\n",
        "        self.base_class = base_class\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.name)\n",
        "\n",
        "class SGUnickler(pickle.Unpickler):\n",
        "    def find_class(self, module, name):\n",
        "        # print(module, name)\n",
        "        if (module == '__main__' or module == 'roadscene2vec.scene_graph.nodes') and name == 'Node':\n",
        "            return Node\n",
        "        return super().find_class(module, name)"
      ],
      "metadata": {
        "id": "DVEQn_3HE2hF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loads a scene graph from a file & converts it to RustGraph or NetworkX object\n",
        "# sg_file: The path to the file containing the pickled scene graph\n",
        "# convert_to_rustworkx: A boolean flag indicating whether to convert the loaded scene graph to a RustGraph object\n",
        "\n",
        "@lru_cache(maxsize=int(os.getenv('SG_CACHE_SIZE', default='128')))\n",
        "def load_sg(sg_file, convert_to_rustworkx=True):\n",
        "    with open(sg_file, 'rb') as f:\n",
        "        # sg = pickle.load(f)\n",
        "        sg = SGUnickler(f).load()\n",
        "    # convert from networkx to rustworkx for efficiency\n",
        "    if convert_to_rustworkx:\n",
        "        sg = networkx_converter(sg)\n",
        "    return sg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "collapsed": true,
        "id": "gceLynbKIxKY",
        "outputId": "a74d78e1-190b-457a-e803-a7dde6e94b46"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'lru_cache' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-456f7ac79061>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mlru_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SG_CACHE_SIZE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'128'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_sg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msg_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_to_rustworkx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msg_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# sg = pickle.load(f)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGUnickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'lru_cache' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checks if loaded scene graph is same as an empty scene graph\n",
        "def is_empty_sg(sg) -> bool:\n",
        "    if isinstance(sg, str) or isinstance(sg, PosixPath):\n",
        "        sg = load_sg(sg)\n",
        "    return is_isomorphic(sg, EMPTY_SG)"
      ],
      "metadata": {
        "id": "cy9qXdp7OuSV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieves metadata about a scene graph, including\n",
        "# the number of nodes, number of edges, and class counts for the nodes\n",
        "\n",
        "def get_sg_metadata(sg, bypass_cache=False) -> Tuple[int, int]:\n",
        "    if isinstance(sg, str) or isinstance(sg, PosixPath):\n",
        "        if bypass_cache:\n",
        "            sg = load_sg.__wrapped__(sg)\n",
        "        else:\n",
        "            sg = load_sg(sg)\n",
        "    return sg.num_nodes(), sg.num_edges(), get_class_counts(sg)"
      ],
      "metadata": {
        "id": "U_a98GQNXtu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# searches for a scene graph isomorphic to the one provided in sg1_file\n",
        "# within a list of other scene graphs specified by sg_files\n",
        "\n",
        "def find_iso(sg1_file, cluster_keys, sg_files,\n",
        "             check_preconditions: bool = True, timeout: float = -1, verbose: bool = False, leave: bool = True):\n",
        "    \"\"\"Find the cluster that the SG given by sg1_file belongs in, i.e. is isomorphic to\"\"\"\n",
        "    sg1 = load_sg(sg1_file)\n",
        "    with tqdm(total=len(cluster_keys), disable=not verbose, leave=leave) as pbar:\n",
        "        for key2 in cluster_keys:\n",
        "            sg2 = load_sg(sg_files[key2])\n",
        "            if is_isomorphic(sg1, sg2, timeout=timeout, check_preconditions=check_preconditions):\n",
        "                pbar.update(len(cluster_keys))\n",
        "                # return the key as soon as we find it\n",
        "                return key2\n",
        "            pbar.update(1)\n",
        "    # if we have not found a key after looking through all of them, return None\n",
        "    return None"
      ],
      "metadata": {
        "id": "ZpTAOhpeaRYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy\n",
        "from tqdm import tqdm\n",
        "with tqdm(total=100) as pbar:\n",
        "    for i in range(100):\n",
        "        # Do something\n",
        "        pbar.update(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKI71YFSeHgu",
        "outputId": "7aaa4103-3fe8-4b4b-a2b3-067d6dc1ca14"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 370849.16it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# There is a file named empty_sg.pkl in the parent directory\n",
        "# of the current Python file\n",
        "EMPTY_SG = load_sg(Path(__file__).parent / 'empty_sg.pkl')\n",
        "EMPTY_SG_METADATA = get_sg_metadata(EMPTY_SG)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "XPK4TqU-e3nC",
        "outputId": "5997e539-6b92-4b88-8ff2-0f6d979bb8a2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'load_sg' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-170dabf85aee>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mEMPTY_SG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_sg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'empty_sg.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mEMPTY_SG_METADATA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sg_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEMPTY_SG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_sg' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class IsoTimeoutError(Exception):\n",
        "    def __init__(self, file1, file2):\n",
        "        self.file1 = file1\n",
        "        self.file2 = file2\n",
        "\n",
        "    def __str__(self):\n",
        "        return f'\"{self.file1}\", \"{self.file2}\"'\n",
        "\n",
        "    def get_files(self):\n",
        "        return self.file1, self.file2"
      ],
      "metadata": {
        "id": "AjFd78Iih0P0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}